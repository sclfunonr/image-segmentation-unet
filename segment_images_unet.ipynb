{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8aSqAdyFtOKceQlHcTkIK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sclfunonr/image-segmentation-unet/blob/main/segment_images_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YflmONxW7dMQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VUVJegiLEO2n"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.convs = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.convs(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels, n_classes):\n",
        "    super(UNet, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.n_classes = n_classes\n",
        "\n",
        "    self.encoder = nn.ModuleList([\n",
        "        DoubleConv(in_channels, 64),\n",
        "        DoubleConv(64, 128),\n",
        "        DoubleConv(128, 256),\n",
        "        DoubleConv(256, 512),\n",
        "    ])\n",
        "\n",
        "    self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    self.bottleneck = DoubleConv(512, 1024)\n",
        "\n",
        "    self.decoder = nn.ModuleList([\n",
        "        nn.ConvTranspose2d(1024, 512, 2, 2),\n",
        "        DoubleConv(1024, 512),\n",
        "        nn.ConvTranspose2d(512, 256, 2, 2),\n",
        "        DoubleConv(512, 256),\n",
        "        nn.ConvTranspose2d(256, 128, 2, 2),\n",
        "        DoubleConv(256, 128),\n",
        "        nn.ConvTranspose2d(128, 64, 2, 2),\n",
        "        DoubleConv(128, 64)\n",
        "    ])\n",
        "\n",
        "    self.classifier = nn.Conv2d(64, n_classes, 1)\n",
        "    # Add a final upsampling layer to match the input size\n",
        "    # self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoder_features = []\n",
        "    for encoder in self.encoder:\n",
        "      x = encoder(x)\n",
        "      encoder_features.append(x)\n",
        "      x = self.pool(x)\n",
        "\n",
        "    x = self.bottleneck(x)\n",
        "\n",
        "    for idx in range(0, len(self.decoder), 2):\n",
        "      x = self.decoder[idx](x)\n",
        "      encoder_feature = encoder_features.pop()\n",
        "\n",
        "      # Replace padding with interpolation to match sizes\n",
        "      # x = F.interpolate(x, size=encoder_feature.shape[2:], mode='bilinear', align_corners=False)\n",
        "      # print(f\"decoder feature shape: {x.shape} | encoder feature shape: {encoder_feature.shape}\")\n",
        "\n",
        "      # Resize the decoder output to match the encoder feature size\n",
        "      x = F.interpolate(x, size=encoder_feature.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "      # diff_x = encoder_feature.shape[2] - x.shape[2]\n",
        "      # diff_y = encoder_feature.shape[3] - x.shape[3]\n",
        "      # x = F.pad(x, (diff_x // 2, diff_x - diff_x //2, diff_y // 2, diff_y - diff_y // 2))\n",
        "      # print(f\"decoder feature shape after padding: {x.shape}\")\n",
        "\n",
        "      # print(f\"x.shape: {x.shape} \")\n",
        "      # print(f\"encoder_feature shape: {encoder_feature.shape}\")\n",
        "      x = torch.cat((x, encoder_feature), dim=1)\n",
        "      x = self.decoder[idx+1](x)\n",
        "\n",
        "    x = self.classifier(x)\n",
        "    x = F.interpolate(x, size=mask.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "    # print(f\"output feature shape: {x.shape}\")\n",
        "    # Upsample the output to match the input size\n",
        "    # x = self.upsample(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "y-vGOZFT7bR4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install kaggle"
      ],
      "metadata": {
        "id": "OIl0iRAOdA6P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUg5-WrDuSnz",
        "outputId": "89a3cb23-5470-438b-e98a-a11f5bffa805"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c carvana-image-masking-challenge\n",
        "! unzip carvana-image-masking-challenge.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JSVF9-GubIe",
        "outputId": "5bd02bde-0577-445e-9bea-df7fde589529"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading carvana-image-masking-challenge.zip to /content\n",
            "100% 24.4G/24.4G [11:23<00:00, 45.5MB/s]\n",
            "100% 24.4G/24.4G [11:23<00:00, 38.4MB/s]\n",
            "Archive:  carvana-image-masking-challenge.zip\n",
            "  inflating: 29bb3ece3180_11.jpg     \n",
            "  inflating: metadata.csv.zip        \n",
            "  inflating: sample_submission.csv.zip  \n",
            "  inflating: test.zip                \n",
            "  inflating: test_hq.zip             \n",
            "  inflating: train.zip               \n",
            "  inflating: train_hq.zip            \n",
            "  inflating: train_masks.csv.zip     \n",
            "  inflating: train_masks.zip         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "if not os.path.isdir(\"/content/train\") or (len(os.listdir(\"/content/train\")) <= 1):\n",
        "  with zipfile.ZipFile(\"/content/train.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "if not os.path.isdir(\"/content/train_masks\") or (len(os.listdir(\"/content/train_masks\")) <= 1):\n",
        "  with zipfile.ZipFile(\"/content/train_masks.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")"
      ],
      "metadata": {
        "id": "3QxmviEOPsVH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize the images and masks\n",
        "def visualize_images(images, masks, num_images=1):\n",
        "  for i in range(num_images):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(images[i].permute(1, 2, 0))\n",
        "    plt.title(\"Image\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(masks[i].permute(1, 2, 0))\n",
        "    plt.title(\"Mask\")\n",
        "    plt.show()\n",
        "\n",
        "images = sorted([\"content\"+\"/train/\" + i for i in os.listdir(\"content\"+\"/train/\")])\n",
        "masks = sorted([\"content\"+\"/train_masks/\" + i for i in os.listdir(\"content\"+\"/train_masks/\")])\n",
        "visualize_images(images, masks, num_images=1)"
      ],
      "metadata": {
        "id": "H4NghExIq5DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CarvanaDataset(Dataset):\n",
        "  def __init__(self, root_path, limit=None):\n",
        "    # Assign values for self.root_path, self.limit, self.images, and self.masks\n",
        "    self.root_path = root_path\n",
        "    self.limit = limit\n",
        "\n",
        "    self.images = sorted([root_path+\"/train/\" + i for i in os.listdir(root_path+\"/train/\")])[:limit]\n",
        "    self.masks = sorted([root_path+\"/train_masks/\" + i for i in os.listdir(root_path+\"/train_masks/\")])[:limit]\n",
        "    if self.limit is None:\n",
        "      self.limit = len(self.images)\n",
        "\n",
        "    self.transform = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor()])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.images[index]).convert(\"RGB\")\n",
        "    mask = Image.open(self.masks[index]).convert(\"L\")\n",
        "    return self.transform(img), self.transform(mask)\n",
        "\n",
        "  def __len__(self):\n",
        "    return min(len(self.images), self.limit)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "btm5v2ihK11g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator().manual_seed(25)\n",
        "train_dataset = CarvanaDataset(\"/content/\")\n",
        "\n",
        "train_dataset, test_dataset = random_split(train_dataset, [0.8, 0.2], generator=generator)\n",
        "test_dataset, val_dataset = random_split(test_dataset, [0.5,0.5], generator=generator)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_workers = torch.cuda.device_count() * 4 if device == \"cuda\" else 0"
      ],
      "metadata": {
        "id": "Q8ep4V3odgqj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 3e-4 # 3x10^-4\n",
        "BATCH_SIZE = 8 # instead of 8\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
        "\n",
        "# pinnned memory is a region of RAM that the OS is prevented from swapping out to disk.\n",
        "model = UNet(in_channels=3, n_classes=1).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "_LJfjjB2WJ4b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DICE score = 2 * |A ∩ B| / (|A| + |B|) -- metric, evaluate segmentation performance\n",
        "def dice_coefficient(prediction, target, epsilon=1e-07):\n",
        "  # print(f\"INSIDE dice_coefficient() prediction.shape: {prediction.shape}\")\n",
        "  # print(f\"INSIDE dice_coefficient() target (mask)shape: {target.shape}\")\n",
        "  prediction_copy = prediction.clone()\n",
        "\n",
        "  prediction_copy[prediction_copy < 0] = 0\n",
        "  prediction_copy[prediction_copy > 0] = 1\n",
        "\n",
        "  intersection = abs(torch.sum(prediction_copy * target)) # \"* \"is element-wise operation\n",
        "  union = abs(torch.sum(prediction_copy) + torch.sum(target))\n",
        "  dice = (2. * intersection + epsilon) / (union + epsilon)\n",
        "  return dice"
      ],
      "metadata": {
        "id": "BHogWVqhZ-6H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "EPOCHS = 5\n",
        "\n",
        "train_losses = []\n",
        "train_dcs = []\n",
        "val_losses = []\n",
        "val_dcs = []\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  model.train()\n",
        "  train_running_loss = 0\n",
        "  train_running_dc = 0\n",
        "\n",
        "  # print(\"\\n\\n------- BEGIN TRAINING ONE EPOCH---------\\n\\n\")\n",
        "\n",
        "  for idx, (image, mask) in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
        "    image = image.float().to(device)\n",
        "    mask = mask.float().to(device)\n",
        "    # print(\"\\n\")\n",
        "    # print(f\"idx: {idx}| image.shape:{image.shape}|mask.shape: {mask.shape}\")\n",
        "\n",
        "    y_pred = model(image)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # print(f\"Before calling dice_coefficient() y_pred shape: mask shape| {y_pred.shape}:{mask.shape}\")\n",
        "    dc = dice_coefficient(y_pred, mask)\n",
        "    loss = loss_fn(y_pred, mask)\n",
        "\n",
        "    train_running_loss += loss.item() # loss.item() moved loss to CPU from GPU and get a python float\n",
        "    train_running_dc += dc.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss = train_running_loss / (idx + 1)\n",
        "  train_dc = train_running_dc / (idx + 1)\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  train_dcs.append(train_dc)\n",
        "  # print(\"\\n\\n------- END TRAINING ONE EPOCH---------\\n\\n\")\n",
        "  # print(\"\\n\\n------- BEGIN VALIDATION ONE EPOCH---------\\n\\n\")\n",
        "  model.eval()\n",
        "  val_running_loss = 0\n",
        "  val_running_dc = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (image, mask) in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n",
        "      image = image.float().to(device)\n",
        "      mask = mask.float().to(device)\n",
        "\n",
        "      y_pred = model(image)\n",
        "      loss = loss_fn(y_pred, mask)\n",
        "      dc = dice_coefficient(y_pred, mask)\n",
        "\n",
        "      val_running_loss += loss.item()\n",
        "      val_running_dc += dc.item()\n",
        "\n",
        "    val_loss = val_running_loss / (idx + 1)\n",
        "    val_dc = val_running_dc / (idx + 1)\n",
        "\n",
        "  val_losses.append(val_loss)\n",
        "  val_dcs.append(val_dc)\n",
        "  # print(\"\\n\\n------- END VALIDATION ONE EPOCH ---------\\n\\n\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"-\" * 30)\n",
        "  print(f\"Training Loss EPOCH {epoch + 1}: {train_loss:.4f}\")\n",
        "  print(f\"Training DICE EPOCH {epoch + 1}: {train_dc:.4f}\")\n",
        "  print(\"\\n\")\n",
        "  print(f\"Validation Loss EPOCH {epoch + 1}: {val_loss:.4f}\")\n",
        "  print(f\"Validation DICE EPOCH {epoch + 1}: {val_dc:.4f}\")\n",
        "  print(\"-\" * 30)\n",
        "\n",
        "# Saving the model\n",
        "torch.save(model.state_dict(), 'my_checkpoint.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "EQrn7RhLbhiX",
        "outputId": "e7c5a0c9-075c-4b5b-d520-6c0227a7909e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 509/509 [13:14<00:00,  1.56s/it]\n",
            "100%|██████████| 64/64 [00:37<00:00,  1.71it/s]\n",
            " 20%|██        | 1/5 [13:51<55:27, 831.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "------------------------------\n",
            "Training Loss EPOCH 1: 0.0402\n",
            "Training DICE EPOCH 1: 0.9630\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 1: 0.0443\n",
            "Validation DICE EPOCH 1: 0.9589\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 91/509 [02:23<10:57,  1.57s/it]\n",
            " 20%|██        | 1/5 [16:15<1:05:00, 975.01s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9d579e423100>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtrain_running_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss.item() moved loss to CPU from GPU and get a python float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mtrain_running_dc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_list = list(range(1, EPOCHS + 1))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs_list, train_losses, label='Training Loss')\n",
        "plt.plot(epochs_list, val_losses, label='Validation Loss')\n",
        "plt.xticks(ticks=list(range(1, EPOCHS + 1, 1)))\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs_list, train_dcs, label='Training DICE')\n",
        "plt.plot(epochs_list, val_dcs, label='Validation DICE')\n",
        "plt.xticks(ticks=list(range(1, EPOCHS + 1, 1)))\n",
        "plt.title('DICE Coefficient over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('DICE')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-FQtZRFVL26R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zoomed loss over epochs\n",
        "epochs_list = list(range(1, EPOCHS + 1))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(epochs_list, train_losses, label='Training Loss')\n",
        "plt.plot(epochs_list, val_losses, label='Validation Loss')\n",
        "plt.xticks(ticks=list(range(1, EPOCHS + 1, 1)))\n",
        "plt.ylim(0, 0.05)\n",
        "plt.title('Loss over epochs (zoomed)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C4ni5rXs9Jd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test - how the model performs on unseen images\n",
        "model_pth = \"content/my_checkpoint.pth\"\n",
        "trained_model = UNet(in_channels=3, n_classes=1).to(device)\n",
        "trained_model.load_state_dict(torch.load(model_pth, map_location=torch.device(device)))\n"
      ],
      "metadata": {
        "id": "qIOeSKJZhH0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_running_loss = 0\n",
        "test_running_dc = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for idx, img_mask in enumerate(tqdm(test_dataloader, position=0, leave=True)):\n",
        "    img = img_mask[0].float().to(device)\n",
        "    mask = img_mask[1].float().to(device)\n",
        "\n",
        "    y_pred = trained_model(img)\n",
        "    loss = loss_fn(y_pred, mask)\n",
        "    dc = dice_coefficient(y_pred, mask)\n",
        "\n",
        "    test_running_loss += loss.item()\n",
        "    test_running_dc += dc.item()\n",
        "\n",
        "    test_loss = test_running_loss / (idx + 1)\n",
        "    test_dc = test_running_dc / (idx + 1)\n"
      ],
      "metadata": {
        "id": "1EeWQUCLjkKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_images_inference(image_tensors, mask_tensors, image_paths, model_pth, device):\n",
        "    model = UNet(in_channels=3, n_classes=1).to(device)\n",
        "    model.load_state_dict(torch.load(model_pth, map_location=torch.device(device)))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((512, 512))\n",
        "    ])\n",
        "\n",
        "    # Iterate for the images, masks and paths\n",
        "    for image_pth, mask_pth, image_paths in zip(image_tensors, mask_tensors, image_paths):\n",
        "        # Load the image\n",
        "        img = transform(image_pth)\n",
        "\n",
        "        # Predict the imagen with the model\n",
        "        pred_mask = model(img.unsqueeze(0))\n",
        "        pred_mask = pred_mask.squeeze(0).permute(1,2,0)\n",
        "\n",
        "        # Load the mask to compare\n",
        "        mask = transform(mask_pth).permute(1, 2, 0).to(device)\n",
        "\n",
        "        print(f\"Image: {os.path.basename(image_paths)}, DICE coefficient: {round(float(dice_coefficient(pred_mask, mask)),5)}\")\n",
        "\n",
        "        # Show the images\n",
        "        img = img.cpu().detach().permute(1, 2, 0)\n",
        "        pred_mask = pred_mask.cpu().detach()\n",
        "        pred_mask[pred_mask < 0] = 0\n",
        "        pred_mask[pred_mask > 0] = 1\n",
        "\n",
        "        plt.figure(figsize=(15, 16))\n",
        "        plt.subplot(131), plt.imshow(img), plt.title(\"original\")\n",
        "        plt.subplot(132), plt.imshow(pred_mask, cmap=\"gray\"), plt.title(\"predicted\")\n",
        "        plt.subplot(133), plt.imshow(mask, cmap=\"gray\"), plt.title(\"mask\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "dk_QTm39lWqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "n = 10\n",
        "\n",
        "image_tensors = []\n",
        "mask_tensors = []\n",
        "image_paths = []\n",
        "\n",
        "for _ in range(n):\n",
        "    random_index = random.randint(0, len(test_dataloader.dataset) - 1)\n",
        "    random_sample = test_dataloader.dataset[random_index]\n",
        "\n",
        "    image_tensors.append(random_sample[0])\n",
        "    mask_tensors.append(random_sample[1])\n",
        "    image_paths.append(random_sample[2])"
      ],
      "metadata": {
        "id": "tQi0BTffmcv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pth = \"/content/my_checkpoint.pth\"\n",
        "random_images_inference(image_tensors, mask_tensors, image_paths, model_pth, device=\"cpu\")"
      ],
      "metadata": {
        "id": "HhXdztBumhXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}